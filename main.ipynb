{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first pipeline we have used - direct mediapipe detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Initialize MediaPipe Hand Detection\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize Color Deques\n",
    "bpoints, gpoints, rpoints, ypoints = [deque(maxlen=1024) for _ in range(4)]\n",
    "blue_index, green_index, red_index, yellow_index = 0, 0, 0, 0\n",
    "\n",
    "# Color Palette\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (0, 255, 255)]\n",
    "colorIndex = 0\n",
    "\n",
    "# Setup Paint Window\n",
    "paintWindow = np.ones((471, 636, 3)) * 255\n",
    "buttons = [(40, \"CLEAR\", (0, 0, 0)), (160, \"BLUE\", (255, 0, 0)), \n",
    "           (275, \"GREEN\", (0, 255, 0)), (390, \"RED\", (0, 0, 255)), \n",
    "           (505, \"YELLOW\", (0, 255, 255))]\n",
    "\n",
    "for x, text, color in buttons:\n",
    "    cv2.rectangle(paintWindow, (x, 1), (x + 100, 65), color, 2)\n",
    "    cv2.putText(paintWindow, text, (x + 10, 33), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "cv2.namedWindow('Paint', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "# Initialize Mediapipe Hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "prev_time = 0  # For FPS calculation\n",
    "writing_enabled = False  # Track if writing is active\n",
    "\n",
    "def distance(a, b):\n",
    "    \"\"\"Calculate Euclidean distance between two points.\"\"\"\n",
    "    return np.linalg.norm(np.array([a.x, a.y]) - np.array([b.x, b.y]))\n",
    "\n",
    "def classify_hand(landmarks):\n",
    "    \"\"\"Classifies hand gestures.\"\"\"\n",
    "    finger_tips = [landmarks[i].y for i in [8, 12, 16, 20]]  # Index, Middle, Ring, Pinky\n",
    "    wrist_y = landmarks[0].y\n",
    "\n",
    "    # Check if only index finger is extended\n",
    "    index_tip = landmarks[8].y\n",
    "    index_distances = [distance(landmarks[i], landmarks[i - 1]) for i in [8, 7, 6]]\n",
    "    avg_index_spread = np.mean(index_distances)\n",
    "\n",
    "    other_finger_spreads = [distance(landmarks[i], landmarks[i - 1]) for i in [12, 16, 20]]\n",
    "    avg_other_finger_spread = np.mean(other_finger_spreads)\n",
    "\n",
    "    if index_tip < wrist_y and avg_index_spread > avg_other_finger_spread * 1.5:\n",
    "        return \"Writing Hand - Continue\", (0, 255, 0)  # Green\n",
    "\n",
    "    if all(tip < wrist_y for tip in finger_tips):\n",
    "        return \"Open Palm - Stop Writing\", (0, 0, 255)  # Red\n",
    "\n",
    "    return None, (255, 255, 255)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Preprocessing\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Convert to RGB for Mediapipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    # Draw UI\n",
    "    for x, text, color in buttons:\n",
    "        cv2.rectangle(frame, (x, 1), (x + 100, 65), color, 2)\n",
    "        cv2.putText(frame, text, (x + 10, 33), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Gesture classification\n",
    "            label, color = classify_hand(hand_landmarks.landmark)\n",
    "            if label:\n",
    "                cv2.putText(frame, label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "            # Update writing status\n",
    "            if label == \"Writing Hand - Continue\":\n",
    "                writing_enabled = True\n",
    "            elif label == \"Open Palm - Stop Writing\":\n",
    "                writing_enabled = False\n",
    "\n",
    "            # Extract fingertip position\n",
    "            index_finger = (int(hand_landmarks.landmark[8].x * frame.shape[1]), \n",
    "                            int(hand_landmarks.landmark[8].y * frame.shape[0]))\n",
    "            \n",
    "            cv2.circle(frame, index_finger, 5, (0, 255, 0), -1)\n",
    "\n",
    "            # Button clicks\n",
    "            if index_finger[1] <= 65:\n",
    "                if 40 <= index_finger[0] <= 140:  # Clear Button\n",
    "                    bpoints, gpoints, rpoints, ypoints = [deque(maxlen=512) for _ in range(4)]\n",
    "                    paintWindow[67:, :, :] = 255\n",
    "                elif 160 <= index_finger[0] <= 255:\n",
    "                    colorIndex = 0  # Blue\n",
    "                elif 275 <= index_finger[0] <= 370:\n",
    "                    colorIndex = 1  # Green\n",
    "                elif 390 <= index_finger[0] <= 485:\n",
    "                    colorIndex = 2  # Red\n",
    "                elif 505 <= index_finger[0] <= 600:\n",
    "                    colorIndex = 3  # Yellow\n",
    "\n",
    "            elif writing_enabled:  # Only draw when writing is enabled\n",
    "                # Ensure each list has at least one deque\n",
    "                while len(bpoints) <= blue_index:\n",
    "                    bpoints.append(deque(maxlen=512))\n",
    "                while len(gpoints) <= green_index:\n",
    "                    gpoints.append(deque(maxlen=512))\n",
    "                while len(rpoints) <= red_index:\n",
    "                    rpoints.append(deque(maxlen=512))\n",
    "                while len(ypoints) <= yellow_index:\n",
    "                    ypoints.append(deque(maxlen=512))\n",
    "\n",
    "                # Append points safely\n",
    "                if colorIndex == 0:\n",
    "                    bpoints[-1].appendleft(index_finger)\n",
    "                elif colorIndex == 1:\n",
    "                    gpoints[-1].appendleft(index_finger)\n",
    "                elif colorIndex == 2:\n",
    "                    rpoints[-1].appendleft(index_finger)\n",
    "                elif colorIndex == 3:\n",
    "                    ypoints[-1].appendleft(index_finger)\n",
    "\n",
    "    # Draw lines\n",
    "    for points, color in zip([bpoints, gpoints, rpoints, ypoints], colors):\n",
    "        for stroke in points:\n",
    "            for i in range(1, len(stroke)):\n",
    "                if stroke[i - 1] is None or stroke[i] is None:\n",
    "                    continue\n",
    "                cv2.line(frame, stroke[i - 1], stroke[i], color, 2)\n",
    "                cv2.line(paintWindow, stroke[i - 1], stroke[i], color, 2)\n",
    "\n",
    "    # Calculate FPS\n",
    "    curr_time = time.time()\n",
    "    fps = int(1 / (curr_time - prev_time)) if prev_time else 0\n",
    "    prev_time = curr_time\n",
    "\n",
    "    cv2.putText(frame, f\"FPS: {fps}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"Air Canvas\", frame)\n",
    "    cv2.imshow(\"Paint\", paintWindow)\n",
    "    cv2.imshow(\"Preprocessed Grayscale\", gray)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The second pipeline, train a model and then add it into the air-canvas model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "data = []\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(rgb_frame)\n",
    "\n",
    "        if result.multi_hand_landmarks:\n",
    "            for hand_landmarks in result.multi_hand_landmarks:\n",
    "                landmarks = [coord for landmark in hand_landmarks.landmark for coord in (landmark.x, landmark.y)]\n",
    "                cv2.imshow(\"Hand Tracking\", frame)\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('o'):  # Open palm\n",
    "                    data.append(landmarks + [0])\n",
    "                elif key == ord('w'):  # Writing hand\n",
    "                    data.append(landmarks + [1])\n",
    "                elif key == ord('q'):  # Quit and save\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df.to_csv(\"hand_data.csv\", index=False)\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    exit()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained with 100.00% accuracy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv(\"hand_data.csv\")\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(model, \"hand_model.pkl\")\n",
    "\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Model trained with {accuracy * 100:.2f}% accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "model = joblib.load(\"hand_model.pkl\")\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(rgb_frame)\n",
    "\n",
    "        if result.multi_hand_landmarks:\n",
    "            for hand_landmarks in result.multi_hand_landmarks:\n",
    "                landmarks = np.array([coord for landmark in hand_landmarks.landmark for coord in (landmark.x, landmark.y)]).reshape(1, -1)\n",
    "                prediction = model.predict(landmarks)[0]\n",
    "\n",
    "                if prediction == 0:\n",
    "                    cv2.putText(frame, \"Open Palm - Stop Writing\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                else:\n",
    "                    cv2.putText(frame, \"Writing Hand - Continue\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"AI Hand Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"hand_data.csv\")\n",
    "\n",
    "# Extract Open Palm & Writing Hand Samples\n",
    "open_palm_samples = df[df.iloc[:, -1] == 0].iloc[:, :-1].values[0].reshape(-1, 2)\n",
    "writing_hand_samples = df[df.iloc[:, -1] == 1].iloc[:, :-1].values[0].reshape(-1, 2)\n",
    "\n",
    "# Create blank images for visualization\n",
    "image_size = 500\n",
    "open_palm_img = np.ones((image_size, image_size, 3), dtype=np.uint8) * 255\n",
    "writing_hand_img = np.ones((image_size, image_size, 3), dtype=np.uint8) * 255\n",
    "\n",
    "def draw_landmarks(image, landmarks, color):\n",
    "    for x, y in landmarks:\n",
    "        x, y = int(x * image_size), int(y * image_size)  # Scale to image size\n",
    "        cv2.circle(image, (x, y), 5, color, -1)\n",
    "\n",
    "# Draw landmarks\n",
    "draw_landmarks(open_palm_img, open_palm_samples, (0, 0, 255))  # Red for Open Palm\n",
    "draw_landmarks(writing_hand_img, writing_hand_samples, (255, 0, 0))  # Blue for Writing Hand\n",
    "\n",
    "# Show images\n",
    "cv2.imshow(\"Open Palm Gesture\", open_palm_img)\n",
    "cv2.imshow(\"Writing Hand Gesture\", writing_hand_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import joblib\n",
    "from collections import deque\n",
    "\n",
    "# Load trained hand gesture model\n",
    "model = joblib.load(\"hand_model.pkl\")\n",
    "\n",
    "# Initialize Mediapipe\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize OpenCV paint parameters\n",
    "bpoints, gpoints, rpoints, ypoints = [deque(maxlen=1024) for _ in range(4)]\n",
    "blue_index, green_index, red_index, yellow_index = 0, 0, 0, 0\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (0, 255, 255)]\n",
    "colorIndex = 0\n",
    "\n",
    "# Setup Paint Canvas\n",
    "paintWindow = np.ones((471, 636, 3), dtype=np.uint8) * 255\n",
    "buttons = [(\"CLEAR\", (40, 1), (140, 65)), (\"BLUE\", (160, 1), (255, 65)),\n",
    "           (\"GREEN\", (275, 1), (370, 65)), (\"RED\", (390, 1), (485, 65)), \n",
    "           (\"YELLOW\", (505, 1), (600, 65))]\n",
    "\n",
    "for text, start, end in buttons:\n",
    "    paintWindow = cv2.rectangle(paintWindow, start, end, (0, 0, 0), 2)\n",
    "    cv2.putText(paintWindow, text, (start[0] + 10, 33), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "cv2.namedWindow('Paint', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Draw buttons\n",
    "    for text, start, end in buttons:\n",
    "        frame = cv2.rectangle(frame, start, end, (0, 0, 0), 2)\n",
    "        cv2.putText(frame, text, (start[0] + 10, 33), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Process hand landmarks\n",
    "    result = hands.process(framergb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            landmarks = np.array([coord for lm in handslms.landmark for coord in (lm.x, lm.y)]).reshape(1, -1)\n",
    "            prediction = model.predict(landmarks)[0]  # Predict gesture\n",
    "\n",
    "            # Drawing landmarks\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "            fore_finger = (int(handslms.landmark[8].x * frame.shape[1]), int(handslms.landmark[8].y * frame.shape[0]))\n",
    "\n",
    "            if prediction == 0:  # Open Palm - Stop Writing\n",
    "                cv2.putText(frame, \"Open Palm - Stop Writing\", (50, 50), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            else:  # Writing Hand - Continue Drawing\n",
    "                cv2.putText(frame, \"Writing Hand - Continue\", (50, 50), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Handle drawing logic\n",
    "                if fore_finger[1] <= 65:  # Check button clicks\n",
    "                    if 40 <= fore_finger[0] <= 140:  # Clear button\n",
    "                        bpoints, gpoints, rpoints, ypoints = [deque(maxlen=512) for _ in range(4)]\n",
    "                        blue_index, green_index, red_index, yellow_index = 0, 0, 0, 0\n",
    "                        paintWindow[67:, :, :] = 255\n",
    "                    elif 160 <= fore_finger[0] <= 255:\n",
    "                        colorIndex = 0  # Blue\n",
    "                    elif 275 <= fore_finger[0] <= 370:\n",
    "                        colorIndex = 1  # Green\n",
    "                    elif 390 <= fore_finger[0] <= 485:\n",
    "                        colorIndex = 2  # Red\n",
    "                    elif 505 <= fore_finger[0] <= 600:\n",
    "                        colorIndex = 3  # Yellow\n",
    "                else:\n",
    "                    if colorIndex == 0:\n",
    "                        if len(bpoints) <= blue_index:\n",
    "                            bpoints.append(deque(maxlen=512))\n",
    "                        bpoints[blue_index].appendleft(fore_finger)\n",
    "                    elif colorIndex == 1:\n",
    "                        if len(gpoints) <= green_index:\n",
    "                            gpoints.append(deque(maxlen=512))\n",
    "                        gpoints[green_index].appendleft(fore_finger)\n",
    "                    elif colorIndex == 2:\n",
    "                        if len(rpoints) <= red_index:\n",
    "                            rpoints.append(deque(maxlen=512))\n",
    "                        rpoints[red_index].appendleft(fore_finger)\n",
    "                    elif colorIndex == 3:\n",
    "                        if len(ypoints) <= yellow_index:\n",
    "                            ypoints.append(deque(maxlen=512))\n",
    "                        ypoints[yellow_index].appendleft(fore_finger)\n",
    "\n",
    "    # Draw painting strokes\n",
    "    points = [bpoints, gpoints, rpoints, ypoints]\n",
    "    for i, point_set in enumerate(points):\n",
    "        for j, point_list in enumerate(point_set):\n",
    "            for k in range(1, len(point_list)):\n",
    "                if point_list[k - 1] is None or point_list[k] is None:\n",
    "                    continue\n",
    "                cv2.line(frame, point_list[k - 1], point_list[k], colors[i], 2)\n",
    "                cv2.line(paintWindow, point_list[k - 1], point_list[k], colors[i], 2)\n",
    "\n",
    "    # Show output\n",
    "    cv2.imshow(\"Output\", frame)\n",
    "    cv2.imshow(\"Paint\", paintWindow)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The observations from comparing the two pipelines:\n",
    "\n",
    "The first pipeline, i.e., Enhancing the hand detection by using mediapipe - is much better than the second pipeline i.e., Training a model using Mediapipe, Random Forest in many terms.\n",
    "\n",
    "We have seen that open palm - stop writing, or writing hand - continue writing work much efficiently and is fast in pipeline 1 when compared to pipeline 2.\n",
    "\n",
    "This is beacuse we need to get the predictions from the model in pipeline 2 which takes a bit of time , but in pipeline 1 it is almost instantanoues.\n",
    "\n",
    "There were still some problems that we could not resolve, like the camera's low configurations causing the hands or mediapipe indexes to fluctuate and causing errors in hand detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
